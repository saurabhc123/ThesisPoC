Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
17/03/29 22:47:19 INFO Remoting: Starting remoting
17/03/29 22:47:19 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriver@172.30.202.195:58799]
Hello World!11
Reset successful
***************** Parameters used for EXPERIMENT *****************
	filterToUse = CosineSim
	minSimilarityThreshold = 0.5
	cosineSimilarityWindowSize= 0.2
	minWmDistanceThreshold = 0.49
	vectorType= google
	webWord2VecBaseUri = http://localhost:5000/getvector/
	cnnClassifierBaseUri = http://localhost:5000/cnn_train_and_get_prediction_labels
	maxFpmWordsToPick = 35
	minFpmWordsDetected = 0
	refreshLocalWordVectors = false
	maxExperimentIterations = 20
	maxAuxTweetsToAddEachIteration = 20
	thresholdF1 = 0.98
	classifierType = Cnn
	folderNameForCnnClassifier = 
	auxiliaryThresholdExpectation = 0.01
	fileDelimiter = ,
	experimentSet = obesity
	experimentSetNumber = 
	trainingDataFile = data/final/obesity_training_data.txt
	validationDataFile = data/final/obesity_validation_data.txt
	auxiliaryDataFile = data/final/obesity_auxiliary_data.txt
	supplementedCleanAuxiliaryFile = data/final/obesity_auxiliary_data.txt

***************** Parameters used for EXPERIMENT *****************

Getting CNN classification labels from http://localhost:5000/cnn_train_and_get_prediction_labels?trainingFolder=e214a59e-7ca2-4525-82e2-91018fa835eb-obesity-Iteration-0&ngram=2

Initial F1=0.96
Precision=0.99
Recall=0.91
Initial - ConfusionMatrix:
104.0  1.0   
7.0    72.0  

Issue with Tweet:, 293
Issue with Tweet:, 294
Issue with Tweet:, 295
Issue with Tweet:, 296
[Stage 22:>                                                         (0 + 8) / 8][Stage 22:==============>                                           (2 + 6) / 8][Stage 22:=====================>                                    (3 + 5) / 8][Stage 22:===========================================>              (6 + 2) / 8]                                                                                Exception in thread "main" org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 23.0 failed 1 times, most recent failure: Lost task 0.0 in stage 23.0 (TID 150, localhost): java.lang.IllegalStateException: SparkContext has been shutdown
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1805)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1826)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1839)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1910)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:905)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:147)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:108)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:306)
	at org.apache.spark.rdd.RDD.collect(RDD.scala:904)
	at Implementations.FeatureGeneratorImpl.FpmBasedMissingWordProvider$.Init(FpmBasedMissingWordProvider.scala:88)
	at Implementations.FeatureGeneratorImpl.FpmBasedMissingWordProvider.replaceWord(FpmBasedMissingWordProvider.scala:22)
	at Implementations.FeatureGeneratorImpl.MissingWordFeatureProcessor$.getMissingWord(MissingWordFeatureProcessor.scala:39)
	at Implementations.FeatureGeneratorImpl.MissingWordFeatureProcessor$.getWord(MissingWordFeatureProcessor.scala:52)
	at Implementations.FeatureGeneratorImpl.MissingWordFeatureProcessor$$anonfun$1.apply(MissingWordFeatureProcessor.scala:17)
	at Implementations.FeatureGeneratorImpl.MissingWordFeatureProcessor$$anonfun$1.apply(MissingWordFeatureProcessor.scala:17)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)
	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:108)
	at Implementations.FeatureGeneratorImpl.MissingWordFeatureProcessor.replaceMissingWords(MissingWordFeatureProcessor.scala:17)
	at Implementations.FeatureGeneratorImpl.WebServiceBasedWordVectorGenerator$$anonfun$1.apply(WebServiceBasedWordVectorGenerator.scala:26)
	at Implementations.FeatureGeneratorImpl.WebServiceBasedWordVectorGenerator$$anonfun$1.apply(WebServiceBasedWordVectorGenerator.scala:26)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at scala.collection.Iterator$$anon$14.hasNext(Iterator.scala:389)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$class.foreach(Iterator.scala:727)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
	at org.apache.spark.util.StatCounter.merge(StatCounter.scala:53)
	at org.apache.spark.util.StatCounter.<init>(StatCounter.scala:35)
	at org.apache.spark.util.StatCounter$.apply(StatCounter.scala:144)
	at org.apache.spark.rdd.DoubleRDDFunctions$$anonfun$stats$1$$anonfun$apply$1.apply(DoubleRDDFunctions.scala:42)
	at org.apache.spark.rdd.DoubleRDDFunctions$$anonfun$stats$1$$anonfun$apply$1.apply(DoubleRDDFunctions.scala:42)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$17.apply(RDD.scala:706)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$17.apply(RDD.scala:706)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:264)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:88)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1280)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1268)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1267)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1267)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:697)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:697)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:697)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1493)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1455)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1444)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:567)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1813)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1933)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1003)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:147)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:108)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:306)
	at org.apache.spark.rdd.RDD.reduce(RDD.scala:985)
	at org.apache.spark.rdd.DoubleRDDFunctions$$anonfun$stats$1.apply(DoubleRDDFunctions.scala:42)
	at org.apache.spark.rdd.DoubleRDDFunctions$$anonfun$stats$1.apply(DoubleRDDFunctions.scala:42)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:147)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:108)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:306)
	at org.apache.spark.rdd.DoubleRDDFunctions.stats(DoubleRDDFunctions.scala:41)
	at org.apache.spark.rdd.DoubleRDDFunctions$$anonfun$mean$1.apply$mcD$sp(DoubleRDDFunctions.scala:47)
	at org.apache.spark.rdd.DoubleRDDFunctions$$anonfun$mean$1.apply(DoubleRDDFunctions.scala:47)
	at org.apache.spark.rdd.DoubleRDDFunctions$$anonfun$mean$1.apply(DoubleRDDFunctions.scala:47)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:147)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:108)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:306)
	at org.apache.spark.rdd.DoubleRDDFunctions.mean(DoubleRDDFunctions.scala:46)
	at Implementations.AuxiliaryDataRetrievers.CosineSimAuxiliaryFilter$$anonfun$3.apply(CosineSimAuxiliaryFilter.scala:36)
	at Implementations.AuxiliaryDataRetrievers.CosineSimAuxiliaryFilter$$anonfun$3.apply(CosineSimAuxiliaryFilter.scala:30)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)
	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:108)
	at Implementations.AuxiliaryDataRetrievers.CosineSimAuxiliaryFilter.filterData(CosineSimAuxiliaryFilter.scala:30)
	at Implementations.AuxiliaryDataRetrievers.CosineSimAuxiliaryFilter.filter(CosineSimAuxiliaryFilter.scala:16)
	at main.scala.Implementations.AuxiliaryDataBasedExperiment.performExperiment(AuxiliaryDataBasedExperiment.scala:135)
	at main.scala.Implementations.AuxiliaryDataBasedExperiment.SetupAndRunExperiment(AuxiliaryDataBasedExperiment.scala:200)
	at main.scala.program$.main(program.scala:39)
	at main.scala.program.main(program.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at com.intellij.rt.execution.application.AppMain.main(AppMain.java:140)
Caused by: java.lang.IllegalStateException: SparkContext has been shutdown
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1805)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1826)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1839)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1910)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:905)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:147)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:108)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:306)
	at org.apache.spark.rdd.RDD.collect(RDD.scala:904)
	at Implementations.FeatureGeneratorImpl.FpmBasedMissingWordProvider$.Init(FpmBasedMissingWordProvider.scala:88)
	at Implementations.FeatureGeneratorImpl.FpmBasedMissingWordProvider.replaceWord(FpmBasedMissingWordProvider.scala:22)
	at Implementations.FeatureGeneratorImpl.MissingWordFeatureProcessor$.getMissingWord(MissingWordFeatureProcessor.scala:39)
	at Implementations.FeatureGeneratorImpl.MissingWordFeatureProcessor$.getWord(MissingWordFeatureProcessor.scala:52)
	at Implementations.FeatureGeneratorImpl.MissingWordFeatureProcessor$$anonfun$1.apply(MissingWordFeatureProcessor.scala:17)
	at Implementations.FeatureGeneratorImpl.MissingWordFeatureProcessor$$anonfun$1.apply(MissingWordFeatureProcessor.scala:17)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)
	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:108)
	at Implementations.FeatureGeneratorImpl.MissingWordFeatureProcessor.replaceMissingWords(MissingWordFeatureProcessor.scala:17)
	at Implementations.FeatureGeneratorImpl.WebServiceBasedWordVectorGenerator$$anonfun$1.apply(WebServiceBasedWordVectorGenerator.scala:26)
	at Implementations.FeatureGeneratorImpl.WebServiceBasedWordVectorGenerator$$anonfun$1.apply(WebServiceBasedWordVectorGenerator.scala:26)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at scala.collection.Iterator$$anon$14.hasNext(Iterator.scala:389)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$class.foreach(Iterator.scala:727)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
	at org.apache.spark.util.StatCounter.merge(StatCounter.scala:53)
	at org.apache.spark.util.StatCounter.<init>(StatCounter.scala:35)
	at org.apache.spark.util.StatCounter$.apply(StatCounter.scala:144)
	at org.apache.spark.rdd.DoubleRDDFunctions$$anonfun$stats$1$$anonfun$apply$1.apply(DoubleRDDFunctions.scala:42)
	at org.apache.spark.rdd.DoubleRDDFunctions$$anonfun$stats$1$$anonfun$apply$1.apply(DoubleRDDFunctions.scala:42)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$17.apply(RDD.scala:706)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$17.apply(RDD.scala:706)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:264)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:88)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
