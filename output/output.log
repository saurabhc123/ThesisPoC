Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
17/03/30 21:48:08 INFO Remoting: Starting remoting
17/03/30 21:48:08 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriver@172.30.206.144:59245]
Starting experiment at 2017-03-30T21:48:09.042-04:00
Reset successful
***************** Parameters used for EXPERIMENT *****************
	filterToUse = CosineSim
	minSimilarityThreshold = 0.3442857142857143
	cosineSimilarityWindowSize= 0.3442857142857143
	minWmDistanceThreshold = 0.49
	vectorType= local
	webWord2VecBaseUri = http://localhost:5000/file_model/getvector/
	cnnClassifierBaseUri = http://localhost:5000/cnn_train_and_get_prediction_labels_local
	maxFpmWordsToPick = 35
	minFpmWordsDetected = 0
	refreshLocalWordVectors = false
	maxExperimentIterations = 20
	maxAuxTweetsToAddEachIteration = 20
	thresholdF1 = 0.98
	classifierType = Cnn
	folderNameForCnnClassifier = 
	auxiliaryThresholdExpectation = 0.01
	fileDelimiter = ,
	experimentSet = severeweather
	experimentSetNumber = 
	trainingDataFile = data/final/severeweather_training_data.txt
	validationDataFile = data/final/severeweather_validation_data.txt
	auxiliaryDataFile = data/final/severeweather_auxiliary_data.txt
	supplementedCleanAuxiliaryFile = data/final/severeweather_auxiliary_data.txt

***************** Parameters used for EXPERIMENT *****************

Getting CNN classification labels from http://localhost:5000/cnn_train_and_get_prediction_labels_local?trainingFolder=b615a424-7291-412c-8cd1-371909018b23-severeweather-Iteration-0&ngram=1

Initial F1=0.55
Precision=1.0
Recall=0.2
Initial - ConfusionMatrix:
51.0  0.0  
32.0  8.0  

0.15|clinton matt bai first storm punish white mountain
0.23|winter weather storm warn go effect afternoon sunday
0.14|look window nice little snow plenty winter
0.21|winter storm titan good turn doe
0.06|occupywallstreet favorited information tweet
0.02|bernie sander unite state verge become oligarchy occupy ows occupywallstreet
[Stage 58:>                                                         (0 + 2) / 2]0.04|spotlight shin palestinian collaborator occupy ows occupywallstreet occupyhq grassroots
                                                                                0.04|occupywallstreet new trinity church force cancel halloween fest occupy wall fox news
0.06|world focus russia nato buildup military asset near russia occupy ows occupywallstreet
0.11|nyc forecast fri night mostly low temp get smart occupywallstreet sgp glennbeck
0.04|april day awareness political prisoner leonard peltier change topprog occupywallstreet ows occupy anarchist uspolitics
0.02|february day fight back mass surveillance occupyinfo owsinfo revolution anonymous
[Stage 64:=============================>                            (1 + 1) / 2]                                                                                0.01|occupywallst need say gamergate politicise sociology occupywallstreet globalism
0.07|argentina investigate police action hospital protest occupy ows occupywallstreet occupyhq grassroots
0.1|nyc forecast thu night low temp occupywallstreet tcot ocra
-0.01|neworganizing see article member add context
0.06|obama unveil historic climate change plan cut carbon pollution occupy ows occupywallstreet
0.11|nyc forecast sit night low temp occupywallstreet tpp
0.09|nyc forecast tue night low temp occupywallstreet teaparty ocra
Exception in thread "main" org.apache.spark.SparkException: Job cancelled because SparkContext was shut down
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:703)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:702)
	at scala.collection.mutable.HashSet.foreach(HashSet.scala:79)
	at org.apache.spark.scheduler.DAGScheduler.cleanUpAfterSchedulerStop(DAGScheduler.scala:702)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onStop(DAGScheduler.scala:1511)
	at org.apache.spark.util.EventLoop.stop(EventLoop.scala:84)
	at org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:1435)
	at org.apache.spark.SparkContext$$anonfun$stop$7.apply$mcV$sp(SparkContext.scala:1715)
	at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1185)
	at org.apache.spark.SparkContext.stop(SparkContext.scala:1714)
	at org.apache.spark.SparkContext$$anonfun$3.apply$mcV$sp(SparkContext.scala:578)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:264)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ShutdownHookManager.scala:234)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:234)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:234)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1699)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp(ShutdownHookManager.scala:234)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:234)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:234)
	at scala.util.Try$.apply(Try.scala:161)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:234)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:216)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:567)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1813)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1933)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1003)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:147)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:108)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:306)
	at org.apache.spark.rdd.RDD.reduce(RDD.scala:985)
	at org.apache.spark.rdd.DoubleRDDFunctions$$anonfun$stats$1.apply(DoubleRDDFunctions.scala:42)
	at org.apache.spark.rdd.DoubleRDDFunctions$$anonfun$stats$1.apply(DoubleRDDFunctions.scala:42)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:147)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:108)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:306)
	at org.apache.spark.rdd.DoubleRDDFunctions.stats(DoubleRDDFunctions.scala:41)
	at org.apache.spark.rdd.DoubleRDDFunctions$$anonfun$mean$1.apply$mcD$sp(DoubleRDDFunctions.scala:47)
	at org.apache.spark.rdd.DoubleRDDFunctions$$anonfun$mean$1.apply(DoubleRDDFunctions.scala:47)
	at org.apache.spark.rdd.DoubleRDDFunctions$$anonfun$mean$1.apply(DoubleRDDFunctions.scala:47)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:147)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:108)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:306)
	at org.apache.spark.rdd.DoubleRDDFunctions.mean(DoubleRDDFunctions.scala:46)
	at Implementations.AuxiliaryDataRetrievers.CosineSimAuxiliaryFilter$$anonfun$3.apply(CosineSimAuxiliaryFilter.scala:36)
	at Implementations.AuxiliaryDataRetrievers.CosineSimAuxiliaryFilter$$anonfun$3.apply(CosineSimAuxiliaryFilter.scala:30)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)
	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:108)
	at Implementations.AuxiliaryDataRetrievers.CosineSimAuxiliaryFilter.filterData(CosineSimAuxiliaryFilter.scala:30)
	at Implementations.AuxiliaryDataRetrievers.CosineSimAuxiliaryFilter.filter(CosineSimAuxiliaryFilter.scala:16)
	at main.scala.Implementations.AuxiliaryDataBasedExperiment.performExperiment(AuxiliaryDataBasedExperiment.scala:134)
	at main.scala.Implementations.AuxiliaryDataBasedExperiment.SetupAndRunExperiment(AuxiliaryDataBasedExperiment.scala:200)
	at main.scala.program$.main(program.scala:46)
	at main.scala.program.main(program.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at com.intellij.rt.execution.application.AppMain.main(AppMain.java:140)
